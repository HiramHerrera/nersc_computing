{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278e429a-333d-4adf-826f-6db6c9b8b205",
   "metadata": {},
   "source": [
    "# Cluster Computing with NERSC - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82460833-db85-4232-bfa1-0009413f184f",
   "metadata": {},
   "source": [
    "The goal of this notebook is to introduce NERSC and help everyone to learn to work in it. This means that you will get familiar with NERSC's module and queues systems, storage policies, setting up an enviroment, work with jupyter and set a job to the different the different queues.\n",
    "\n",
    "This notebook will be brief, but you can look at NERCS's documentation [here](https://docs.nersc.gov)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b52551-d74c-4878-a110-4d879f952643",
   "metadata": {},
   "source": [
    "## Storage systems\n",
    "\n",
    "NERSC works with different file systems for multiple purposes. Usually, you will be working in either your ```$HOME``` directory  or ```$SCRATCH```. Further information about the storage systems is found [here](https://docs.nersc.gov/filesystems/).\n",
    "\n",
    "- **HOME**: is a permanent storage system with small capacity (40 GB) recommended for code, scripts and stuff you may want to keep. \n",
    "\n",
    "- **SCRACTH** is a large capacity system (20 TB) intended for temporary storage since it has a 12 week purge limit for files that have not been read!.\n",
    "\n",
    "For the moment lets work in SCRATCH.\n",
    "\n",
    "```\n",
    "$ cd $SCRATCH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60263bc1-89f2-4c26-8c1c-2ae44ea3a222",
   "metadata": {},
   "source": [
    "## Modules in NERSC\n",
    "\n",
    "Working on a cluster usually means working with modular systems, NERSC is no exception. The main advantage of this is that you can manage multiple softwares without breaking something (or not so easy!) and work with multiple versions of these softwares too. \n",
    "\n",
    "Let's see what modules you can use.\n",
    "\n",
    "By default you have some modules loaded when logging into your accout, you can check this by typing in your terminal:\n",
    "\n",
    "```\n",
    "$ module list\n",
    "```\n",
    " \n",
    "But, what other modules can you use in NERSC? There's lots of them! \n",
    "\n",
    "You can check all the available modules with\n",
    "\n",
    "```\n",
    "$ module avail\n",
    "```\n",
    "\n",
    "How do I use an specific module? And what does it do? \n",
    "The latter can be checked with ```module show``` command: e.g\n",
    "\n",
    "```\n",
    "$ module show python\n",
    "```\n",
    "\n",
    "The former can be done with ```module load```.\n",
    "\n",
    "\n",
    "Lets load the python module:\n",
    "\n",
    "```\n",
    "$ module load python\n",
    "```\n",
    "\n",
    "You can check with ```module list``` that you now have an additional module.\n",
    "\n",
    "You can find further information about modules at nersc [here](https://docs.nersc.gov/environment/modules/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf5d61-1194-423d-9b88-e54566ffba5f",
   "metadata": {},
   "source": [
    "## Setting up a python enviroment in NERSC\n",
    "\n",
    "Now that we have python's module loaded lets create an enviroment. \n",
    "\n",
    "\n",
    "This is recommended since some codes have conflicts with some specific library versions. To avoid this you can create an enviroment for an specific task you may have. For further information about enviroment manging visit [this](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) page.\n",
    "\n",
    "Lets say that you need to use Python 3.7 and numpy 1.19.0 specifically and any astropy version for a task. This can be done with \n",
    "\n",
    "```\n",
    "$ conda create -n myenv python=3.7 numpy=1.19.0 astropy\n",
    "```\n",
    "\n",
    "Before activating your enviroment type ```pip list```, just to see that your enviroment actually has different stuff than the default NERSC's python enviroment.\n",
    "\n",
    "Now do the following:\n",
    "\n",
    "```\n",
    "$ source activate mynev\n",
    "$ pip list\n",
    "```\n",
    "\n",
    "Now you are working in your specific enviroment! You should notice this by looking at your enviroment's name at the left of your terminal e.g:  ```(myenv)$```.\n",
    "\n",
    "You can exit this enviroment by typing ```conda deactivate```\n",
    "\n",
    "NOTE: Everytime you want to work in your enviroment you should do the following:\n",
    "\n",
    "```\n",
    "$ module load python\n",
    "$ source activate myenv\n",
    "```\n",
    "\n",
    "Sometimes your enviroment requires multiple instructions and might be difficult to remember every step to set it up. You can avoid this struggle by creating a shell script and source it everytime you want to work in this specific enviroment. For example, lets use the shell file aditional to this notebook:\n",
    "\n",
    "```\n",
    "$ source <path/to/script>/load_myenv.sh\n",
    "```\n",
    "\n",
    "This shell file can contain whatever you want! Export paths, definitions, multiple module loading, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee203111-94a0-4d71-a847-ba9ffa9222b2",
   "metadata": {},
   "source": [
    "## Working in Jupyter at NERSC\n",
    "\n",
    "So what if I want to work from a notebook remotely? Fortunatelly we have Jupyter at NERSC! Just go to https://jupyter.nersc.gov/hub/home and log into your account.\n",
    "\n",
    "But what about an specific or custom kernel? This is a bit tricky so lets work on that!\n",
    "\n",
    "First, create a shell file with the specifications to your enviroment. An example is added to this notebook ```myenv.sh```. This file should be an executable. To do this simply run:\n",
    "```\n",
    "$ chmod u+x myenv.sh\n",
    "```\n",
    "\n",
    "Then, create a folder for your enviroment in the kernels in jupyter:\n",
    "\n",
    "```\n",
    "$ cd $HOME/.local/share/jupyter/kernels/\n",
    "$ mkdir myenv\n",
    "$ cd myenv\n",
    "```\n",
    "\n",
    "Finally, create a json file for your kernel. This is usually done with ```vi kernel.sh``` and writing (or copying) the whole thing. But you can just copy the file attached to this notebook. **IMPORTANT:** in this file the second line should point to the ```myenv.sh``` file you created.\n",
    "\n",
    "**NOTE:** This is done just once per kernel.\n",
    "\n",
    "Lets see we are using our enviroment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d211b8-bcd4-47d8-ad94-2c3b2b4f3f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0aebfd-2e58-4325-9c40-db06b38daf15",
   "metadata": {},
   "source": [
    "It works!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e037e-7ffe-428c-996d-0708c8401e70",
   "metadata": {},
   "source": [
    "## Using DESICODE modules\n",
    "\n",
    "If you want to use desicode in a FRESH NEW terminal type the following:\n",
    "\n",
    "```\n",
    "$ source /project/projectdirs/desi/software/desi_environment.sh master\n",
    "```\n",
    "\n",
    "The last part of this line can vary depending on the version of the desicode you want to use, ```master``` uses the current status of the libraries up to date, you can load stable versions, the latest is ```21.5```.\n",
    "\n",
    "To create a Jupyter kernel symply use the folllowing:\n",
    "\n",
    "```\n",
    "$ source /project/projectdirs/desi/software/activate_desi_jupyter.sh master\n",
    "```\n",
    "Once again you can substitute master with any stable version.\n",
    "\n",
    "\n",
    "**NOTE:** activating the jupyter enviroment for DESI should be done only ONCE per version you want to install. I recommend master and any stable version you like.\n",
    "\n",
    "You may want to create a custum DESI enviroment, however this is a quite specific task, so we refer to the [wiki page](https://desi.lbl.gov/trac/wiki/Pipeline/GettingStarted/NERSC#CustomizingYourDESIPythonEnvironment) that helps you with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13113e2-afe3-4e7a-bfea-9fa4f77a10e5",
   "metadata": {},
   "source": [
    "# Submitting Jobs\n",
    "\n",
    "Now that we have set up our working enviroments (or where you will most likely be working) is time to send a job to NERSC.\n",
    "\n",
    "## NERSC queue policy\n",
    "\n",
    "NERSC has different queues with some restrictions according with the requirements of the user. The usual queues you will be working (unless some special requirement) are:\n",
    "\n",
    "- **Regular**: the standard queue for jobs, has a limit in computing time of 48 hours and a maximum of 5000 submitted jobs without run limit. There is no maximum requested nodes limit meaning that you can use up to 1932 for Haswell and 9489 for KNL. Click [here](https://docs.nersc.gov/systems/cori/) for more information about Haswell and KNL.\n",
    "\n",
    "- **Interactive**: Usually used for code development, test, debug and analysis. Has a time limit of 4 hours, can run up to 2 simultaneous jobs and this is also the limit of submitted jobs. There is a maximum requested node number of 64. Jobs in interactive should be run via salloc or srun (we will get to this). \n",
    "\n",
    "- **Debug**: Used for code developement, test and debugging. Has a time limit of 30 minutes, can run up to 2 simultaneous jobs with a 5 submitted jobs limit, and 64 requested nodes limit.\n",
    "\n",
    "For other queues you can check [here](https://docs.nersc.gov/jobs/policy/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246396c0-c149-4e89-986d-82c5b3b00eb3",
   "metadata": {},
   "source": [
    "## Submitting jobs with salloc or srun\n",
    "\n",
    "You can run jobs directly from command line with ```salloc``` or ```srun```. \n",
    "\n",
    "The command ```salloc```is used to allocate resources and work interactively, this is commmonly used with the interactive queue.\n",
    "\n",
    "While ```srun```is used to submit a single job, this instruction can be used inside any batch script, during an allocation in interactive queue or directly from command line.\n",
    "\n",
    "Aditionally to these commands you should use options for the job, this includes the number of nodes, the queue, time and computing system to be used, however there are multiple options that can be set up, these can be found [here](https://docs.nersc.gov/jobs/#commonly-used-options)\n",
    "\n",
    "An example of an salloc command is the following:\n",
    "\n",
    "```\n",
    "$ salloc -A desi -C haswell -q interactive -t 00:10:00 -N 1\n",
    "```\n",
    "\n",
    "In this command we are requesting 10 minutes of one node of haswell in the interactive queue, you can change haswell for knl, ask up to 64 nodes and up to 4 hours. This will prompt a interactive node in your terminal. You should notice it when your terminal has a ```@nid``` number right to your user. Once you are have allocated resources you can run a job inside this interactive session with ```srun```.\n",
    "\n",
    "\n",
    "```\n",
    "$ srun python test_script.py\n",
    "```\n",
    "\n",
    "A thing you should know is that ```srun```inherits the properties from salloc and can add more for example if you ask for 3 nodes with salloc ```-N 3``` you distribute different jobs between them by writing ```srun -N 1 ```. \n",
    "\n",
    "Also ```salloc``` uses the enviroment you are currently working in your terminal when calling it. But you can also load modules or enviroments inside the allocated node.\n",
    "\n",
    "If your job finishes before your allocation time runs out just type ```exit```.\n",
    "\n",
    "Equivalently we could have run ```srun```directly from our command line. \n",
    "\n",
    "\n",
    "```\n",
    "$ srun -A desi -C haswell -q interactive -t 00:10:00 -N 1 python test_script.py\n",
    "```\n",
    "\n",
    "You can change the interactive queue for whatever your needs are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4235734-a869-410b-9fc2-b1be868593b8",
   "metadata": {},
   "source": [
    "## Submitting jobs with SBATCH\n",
    "\n",
    "The last piece of our tutorial is to submit jobs via sbatch, this is done via a bash script like the one attached to this notebook ``èxample_batch.batch```. It must include the options you want with a ```#SBATCH``` flag. Followed by the instructions you want to run. This file can contain anything after the SBATCH flags, like loading modules, calculations via another script, srun commands, for iterations, etc.\n",
    "\n",
    "To run it simply type:\n",
    "\n",
    "```\n",
    "$ sbatch example_batch.batch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ac35d-3da2-4c98-b6c1-17004d1e7f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
